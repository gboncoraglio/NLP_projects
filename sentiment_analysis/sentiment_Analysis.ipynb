{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentiment_Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "momUpfyaTt5s",
        "colab_type": "text"
      },
      "source": [
        "# Sentiment analysis \n",
        "\n",
        "This project involves building a movie review classifier\n",
        "Sentiment analysis is the task of classifying the polarity of a given text.\n",
        "\n",
        "The dataset used is the IMDb dataset. The data was compiled by Andrew Maas (http://ai.stanford.edu/~amaas/data/sentiment/)\n",
        "This dataset contains 50,000 reviews labelled as either positive or negative. Moreover, this datset is balanced, i.e. equal number of postive and negative reviews. Moreover, for each movie, a number equal or less than 30 reviews is considered since reviews tend to correlate.\n",
        "Finally, only reviews with either a low score (less or equal to 4 out of 10) or high score (greater or equal to 7 out of 10) are considered to avoid considering reviews that are neutral.\n",
        "For the purpose of testing, we are going to use only the training data since they are labelled. \n",
        "\n",
        "\n",
        "For this problem, we are going to use a state-of-the-art language model pre-training model, BERT (Bidirectional Encoder Representations from Transformers)model and we will apply transfer learning to adapt the model to the sentiment analysis problem. \n",
        "The model is going to be loaded from the library transformers (https://github.com/huggingface/transformers). We are going to use Pytorch as framework for this problem.\n",
        "\n",
        "## Data processing \n",
        "For the data processing task:\n",
        "- Lowercasing is applied. Lowercasing ALL the text data, is one of the simplest and most effective form of text preprocessing. Some pre-trained model might give a different answers if the input is capitalized or not. Of course, in some specific cases, it might be useful not to change to lowercasing.\n",
        "- Noise Removal is applied. Noise removal is about removing characters digits and pieces of text that can interfere with your text analysis. For instances, punctuations signs, parenthesis, mathematical symbols are removed (for instance, '.' '(' '?' or '<' or ':' are removed).\n",
        "- Since we are using BERT model, we keep the words as they are, i.e. stemming is **not** used. Stemming is the process of reducing inflection in words (e.g. connected, connections) to their root form (e.g. connect).\n",
        "- For the same reason explained before, lemmatization is **not** used. Lemmatization is very similar to stemming. The difference is that, lemmatization doesn’t just chop things off, it actually transforms words to the actual root.\n",
        "\n",
        "Once the data is pre-processed, then the training data (25,000 samples) are used to train the model and testing data (25,000 samples) are used to test the final model.\n",
        "\n",
        "## Building the model\n",
        "The model used is the pre-trained BERT model with a final fully connected layer. \n",
        "Specifically, it is used BERT base model – with a hidden size of 768, with 12 layers (transformer blocks), 12 attention heads, and 110 million parameters.\n",
        "\n",
        "The input to BERT is a sequence of words, and the output is a sequence of vectors.\n",
        "BERT model can accept up to 512 words. However, some review sentences might be longer than this number. In order to deal with this, we apply the strategy described in the paper \"How to Fine-Tune BERT for Text Classification?\" by Sun et al. They propose to use a truncation method, and select part of the beginning and part of the end of the sentence. \n",
        "In fact, the key information of an article is at the beginning and end. Thus, empirically the first 128 and the last 382 tokens are selected if longer than 512.\n",
        "\n",
        "The neural network model will have first the 11 BERT layers, then droput and finally a linear layer. Droput is added for the purpose of Regulariaztion and the linear layer for Classification.\n",
        "\n",
        "\n",
        "## Training the model\n",
        "The final layer is used to compute the loss and determine the accuracy of the model. The loss used is Binary Cross Entropy which is implemented as BCELogits Loss in PyTorch.\n",
        "A learning rate of 2.5e-5 is used. The optimization algorithm used is ADAM with $\\beta_1 = 0.9$ and $\\beta_2 = 0.999$. The batch size is 10 and the number of epochs is 5. \n",
        "\n",
        "## Testing the model\n",
        "Once the model is trained, the algorithm is tested on the test set and reach an accuracy of 99.4% using only 5 epochs! this shows how transfer learning is important for using these models for additional tasks!\n",
        "\n",
        "\n",
        "Thanks,\n",
        "Gabriele Boncoraglio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bUTE_TJXvc0",
        "colab_type": "text"
      },
      "source": [
        "Import some useful libraries for running the code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YOxX0KG3cKs",
        "colab_type": "code",
        "outputId": "af9fde12-6775-4a8b-f840-3912b727a0eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import re,os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score, classification_report\n",
        "import pandas as pd\n",
        "\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "# !pip install -Iv transformers==2.2.2\n",
        "import transformers\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import BertTokenizer, BertModel, BertConfig\n",
        "\n",
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "print(device)\n",
        "\n",
        "os.chdir('/content/drive/My Drive/Colab Notebooks/Sentiment Analysis')\n",
        "#True if you already trained the model\n",
        "trained = False"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dR-KHZltegU_",
        "colab_type": "text"
      },
      "source": [
        "## Data processing "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7XWOJ-4X1j3",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pR3a1Iht5Iu7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = []\n",
        "for line in open('training_dataset.txt', 'r'):\n",
        "    train.append(line.strip())\n",
        "\n",
        "test = []\n",
        "for line in open('testing_dataset.txt', 'r'):\n",
        "    test.append(line.strip())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUEj0eas46s0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data noise removal\n",
        "def preprocess_reviews(reviews):\n",
        "    reviews = [re.compile(\"[.;:!\\'?,\\\"()\\[\\]]\").sub(\"\", line.lower()) for line in reviews]\n",
        "    reviews = [re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
        ".sub(\" \", line) for line in reviews]\n",
        "\n",
        "    return reviews\n",
        "\n",
        "train1 = preprocess_reviews(train)\n",
        "test1 = preprocess_reviews(test)\n",
        "#Creating labels for dataset\n",
        "#first 12500 are postive and the rest is negative\n",
        "target = [1 if i < 12500 else 0 for i in range(25000)]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npMyH0-kgOCd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "9313d163-1735-48f6-8bb0-09870a59892a"
      },
      "source": [
        "# Dealing with long sentences\n",
        "# Looking average and max length of the training sentences dataset\n",
        "maxLen = 0\n",
        "avgLen = 0\n",
        "for stri in train1:\n",
        "  s = stri.split()\n",
        "  maxLen = max(maxLen,len(s))\n",
        "  avgLen += len(s)\n",
        "avgLen /= len(train1)\n",
        "print('(training) average is: ',round(avgLen))\n",
        "print('(training) max length is: ',maxLen)\n",
        "\n",
        "\n",
        "maxLen = 0\n",
        "avgLen = 0\n",
        "for stri in test1:\n",
        "  s = stri.split()\n",
        "  maxLen = max(maxLen,len(s))\n",
        "  avgLen += len(s)\n",
        "avgLen /= len(train1)\n",
        "print('(testing) average is: ',round(avgLen))\n",
        "print('(testing) max length is: ',maxLen)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(training) average is:  233\n",
            "(training) max length is:  2473\n",
            "(testing) average is:  228\n",
            "(testing) max length is:  2245\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEn1_rBW5WHv",
        "colab_type": "code",
        "outputId": "4f95b915-08d3-4ddf-f3af-df6af218ab31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# Visualizing some of the comments and the response to see our data\n",
        "d_train = {'comment_text': train1, 'list': target}\n",
        "d_test = {'comment_text': test1, 'list': target}\n",
        "df_train = pd.DataFrame(data=d_train)\n",
        "df_test = pd.DataFrame(data=d_train)\n",
        "df_train.head()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>comment_text</th>\n",
              "      <th>list</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>bromwell high is a cartoon comedy it ran at th...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>homelessness or houselessness as george carlin...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>brilliant over acting by lesley ann warren bes...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>this is easily the most underrated film inn th...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>this is not the typical mel brooks film it was...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                        comment_text  list\n",
              "0  bromwell high is a cartoon comedy it ran at th...     1\n",
              "1  homelessness or houselessness as george carlin...     1\n",
              "2  brilliant over acting by lesley ann warren bes...     1\n",
              "3  this is easily the most underrated film inn th...     1\n",
              "4  this is not the typical mel brooks film it was...     1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwQSaks2lsET",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "1570b39c-7486-477a-816b-0391b4c82375"
      },
      "source": [
        "\n",
        "# Creating the dataset and dataloader for the neural network\n",
        "train_size = 1.0\n",
        "train_dataset=df_train.sample(frac=train_size,random_state=200).reset_index(drop=True)\n",
        "validation_dataset=df_train.drop(train_dataset.index).reset_index(drop=True)\n",
        "\n",
        "\n",
        "print(\"FULL Dataset: {}\".format(df_train.shape[0]+df_test.shape[0]))\n",
        "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
        "print(\"VALIDATION Dataset: {}\".format(validation_dataset.shape))\n",
        "print(\"TEST Dataset: {}\".format(df_test.shape))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FULL Dataset: 50000\n",
            "TRAIN Dataset: (25000, 2)\n",
            "VALIDATION Dataset: (0, 2)\n",
            "TEST Dataset: (25000, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgInAM1QhxT-",
        "colab_type": "text"
      },
      "source": [
        "## Building the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9aOhBZdW52gs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Sections of config\n",
        "# Defining some key variables that will be used later on in the training\n",
        "MAX_LEN = 512\n",
        "TRAIN_BATCH_SIZE = 10\n",
        "VALID_BATCH_SIZE = 10\n",
        "TEST_BATCH_SIZE = 10\n",
        "EPOCHS = 5\n",
        "LEARNING_RATE = 2e-05\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbwELROF_EXZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "PyTorch supports two different types of datasets:\n",
        "- map-style datasets,\n",
        "- iterable-style datasets.\n",
        "\n",
        "        - Map-style datasets\n",
        "        A map-style dataset is one that implements the __getitem__() and __len__() protocols, \n",
        "        and represents a map from (possibly non-integral) indices/keys to data samples.\n",
        "        For example, such a dataset, when accessed with dataset[idx], \n",
        "        could read the idx-th image and its corresponding label from a folder on the disk.\n",
        "        \n",
        "        - An iterable-style dataset is an instance of a subclass of IterableDataset that \n",
        "        implements the __iter__() protocol, and represents an iterable over data samples. \n",
        "        This type of datasets is particularly suitable for cases where random reads \n",
        "        are expensive or even improbable, and where the batch size depends on the fetched data.\n",
        "        For example, such a dataset, when called iter(dataset), \n",
        "        could return a stream of data reading from a database, a remote server, \n",
        "        or even logs generated in real time.\n",
        "'''\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "\n",
        "    def __init__(self, dataframe, tokenizer, max_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = dataframe\n",
        "        self.comment_text = dataframe.comment_text\n",
        "        self.targets = self.data.list\n",
        "        self.max_len = max_len\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.comment_text)\n",
        "    \n",
        " \n",
        "    def __getitem__(self, index):\n",
        "        comment_text = str(self.comment_text[index])\n",
        "        comment_text = \" \".join(comment_text.split())\n",
        "        if len(comment_text) > 512:\n",
        "          comment_text = comment_text[:128] + comment_text[381:] \n",
        "\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            comment_text,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            stride = 0,\n",
        "            truncation_strategy = 'longest_first',\n",
        "            pad_to_max_length = True,\n",
        "            return_token_type_ids=True\n",
        "        )\n",
        "        ids = inputs['input_ids']\n",
        "        mask = inputs['attention_mask']\n",
        "        token_type_ids = inputs[\"token_type_ids\"]\n",
        "\n",
        "\n",
        "        return {\n",
        "            'ids': torch.tensor(ids, dtype=torch.long),\n",
        "            'mask': torch.tensor(mask, dtype=torch.long),\n",
        "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
        "            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n",
        "        }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BrLFt5Bb_4uW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating a Map-style datasets for PyTorch\n",
        "training_set = CustomDataset(train_dataset, tokenizer, MAX_LEN)\n",
        "validation_set = CustomDataset(test_dataset, tokenizer, MAX_LEN)\n",
        "testing_set = CustomDataset(df_test, tokenizer, MAX_LEN)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOKqHZBpCa52",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "                }\n",
        "validation_params = {'batch_size': VALID_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "test_params = {'batch_size': TEST_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "\n",
        "training_loader = DataLoader(training_set, **train_params)\n",
        "validation_loader = DataLoader(validation_set, **validation_params)\n",
        "testing_loader = DataLoader(testing_set, **test_params)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A697cQy_CiDY",
        "colab_type": "code",
        "outputId": "e66ef6ea-32a1-40fc-c5eb-b73b4d794127",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "'''\n",
        "    We will be creating a neural network with the BERTClass.\n",
        "    This network will have the Bert model. Follwed by a Droput and Linear Layer. \n",
        "    They are added for the purpose of Regulariaztion and Classification respectively.\n",
        "    \n",
        "    Final layer outputs is what will be used to calcuate the loss and \n",
        "    to determine the accuracy of models prediction.\n",
        "    The loss function used will be a combination of Binary Cross Entropy \n",
        "    which is implemented as BCELogits Loss in PyTorch\n",
        "'''\n",
        "# Creating the customized model, by adding a drop out and a dense layer on top of distil bert to get the final output for the model. \n",
        "\n",
        "class BERTClass(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        '''\n",
        "            In the forward loop, there are 2 output from the BertModel layer.\n",
        "            The second output output_1 or called the pooled output is passed to the Drop Out layer and \n",
        "            the subsequent output is given to the Linear layer.\n",
        "            Keep note the number of dimensions for Linear Layer is 6 because that is \n",
        "            the total number of categories in which we are looking to classify our model.\n",
        "        '''\n",
        "        super(BERTClass, self).__init__()\n",
        "        self.l1 = transformers.BertModel.from_pretrained('bert-base-uncased')\n",
        "        # #Freeze bert layers\n",
        "        # for p in self.l1.parameters():\n",
        "        #     p.requires_grad = False\n",
        "        self.l2 = torch.nn.Dropout(0.1)\n",
        "        self.l3 = torch.nn.Linear(768, 1,bias=True)\n",
        "        # self.l4 = torch.nn.Tanh()\n",
        "        # self.l5 = torch.nn.Linear(100, 6,bias=True)\n",
        "    \n",
        "    def forward(self, ids, mask, token_type_ids):\n",
        "        _, output_1= self.l1(ids, attention_mask = mask, token_type_ids = token_type_ids)\n",
        "        output_2 = self.l2(output_1)\n",
        "        output = self.l3(output_2)\n",
        "        # output_4 = self.l4(output_3)\n",
        "        # output = self.l5(output_4)\n",
        "        return output\n",
        "\n",
        "model = BERTClass()\n",
        "\n",
        "if trained:\n",
        "  model.load_state_dict(torch.load(\"bert_sentimentAnaly\"))\n",
        "model.to(device)\n",
        "\n"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BERTClass(\n",
              "  (l1): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (l2): Dropout(p=0.1, inplace=False)\n",
              "  (l3): Linear(in_features=768, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhQDRwXUFuNx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def loss_fn(outputs, targets):\n",
        "    return torch.nn.BCEWithLogitsLoss()(outputs, targets)\n",
        "\n",
        "\n",
        "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE, weight_decay = 5e-4)\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n",
        "def validation(loader):\n",
        "    model.eval()\n",
        "    fin_targets=[]\n",
        "    fin_outputs=[]\n",
        "    with torch.no_grad():\n",
        "        for _, data in enumerate(loader, 0):\n",
        "            ids = data['ids'].to(device, dtype = torch.long)\n",
        "            mask = data['mask'].to(device, dtype = torch.long)\n",
        "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "            targets = data['targets'].to(device, dtype = torch.float)\n",
        "            targets = targets.unsqueeze(1)\n",
        "            outputs = model(ids, mask, token_type_ids)\n",
        "            fin_targets.extend(targets.cpu().detach().numpy().tolist())\n",
        "            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
        "    return fin_outputs, fin_targets\n",
        "\n",
        "def computeScores(testing_loader,typeSet):\n",
        "    outputs, targets = validation(testing_loader) \n",
        "    outputs = np.array(outputs) >= 0.5\n",
        "    accuracy = metrics.accuracy_score(targets, outputs)\n",
        "    f1_score_micro = metrics.f1_score(targets, outputs, average='micro')\n",
        "    f1_score_macro = metrics.f1_score(targets, outputs, average='macro')\n",
        "    print(\"{}\".format(typeSet))\n",
        "    print(f\"  Accuracy Score = {accuracy}\")\n",
        "    print(f\"  F1 Score (Micro) = {f1_score_micro}\")\n",
        "    print(f\"  F1 Score (Macro) = {f1_score_macro}\")\n",
        "    print(\" \")\n",
        "    return outputs, targets\n",
        "    # return accuracy,f1_score_micro,f1_score_macro"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqDk05FgEkTs",
        "colab_type": "code",
        "outputId": "e5920bb1-61d4-4301-9759-46c0cf7b193a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "if not trained:\n",
        "  for epoch in range(EPOCHS):\n",
        "      # ========================================\n",
        "      #               Training\n",
        "      # ========================================\n",
        "      \n",
        "      # Perform one full pass over the training set.\n",
        "\n",
        "      print(\"\")\n",
        "      print('======== Epoch {:} / {:} ========'.format(epoch + 1, EPOCHS))\n",
        "      print('Training...')\n",
        "      # Measure how long the training epoch takes.\n",
        "      t0 = time.time()\n",
        "\n",
        "      # Reset the total loss for this epoch.\n",
        "      total_train_loss = 0\n",
        "\n",
        "      # Put the model into training mode. Don't be mislead--the call to \n",
        "      # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "      # `dropout` and `batchnorm` layers behave differently during training\n",
        "      # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "      model.train()\n",
        "\n",
        "      # for _,data in enumerate(training_loader, 0):\n",
        "      for step, data in enumerate(training_loader):\n",
        "\n",
        "          ids = data['ids'].to(device, dtype = torch.long)\n",
        "          mask = data['mask'].to(device, dtype = torch.long)\n",
        "          token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "          targets = data['targets'].to(device, dtype = torch.float)\n",
        "          targets = targets.unsqueeze(1)\n",
        "          # target = target.float()\n",
        "          outputs = model(ids, mask, token_type_ids)\n",
        "\n",
        "\n",
        "          # Always clear any previously calculated gradients before performing a\n",
        "          # backward pass. PyTorch doesn't do this automatically because \n",
        "          # accumulating the gradients is \"convenient while training RNNs\". \n",
        "          # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "          optimizer.zero_grad()\n",
        "          loss = loss_fn(outputs, targets)\n",
        "\n",
        "          # Progress update every 100 batches.\n",
        "          if step % 100 == 0 and not step == 0:\n",
        "              # Calculate elapsed time in minutes.\n",
        "              elapsed = format_time(time.time() - t0)\n",
        "              \n",
        "              # Report progress.\n",
        "              print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.      Current Loss: {}'.format(step, len(training_loader), elapsed,loss.item()))\n",
        "\n",
        "          # Accumulate the training loss over all of the batches so that we can\n",
        "          # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "          # single value; the `.item()` function just returns the Python value \n",
        "          # from the tensor.\n",
        "          total_train_loss += loss.item()\n",
        "\n",
        "          \n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "      # Calculate the average loss over all of the batches.\n",
        "      avg_train_loss = total_train_loss / len(training_loader)            \n",
        "      \n",
        "      # Measure how long this epoch took.\n",
        "      training_time = format_time(time.time() - t0)\n",
        "\n",
        "      print(\"\")\n",
        "      print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "      print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "      print(\"\")\n",
        "      # computeScores(testing_loader,'Testing set')\n",
        "      # computeScores(validating_loader,'Validation set')\n",
        "      # computeScores(validating_loader,'Validation set')\n",
        "      print(\"\")\n",
        "\n",
        "if not trained:\n",
        "  torch.save(model.state_dict(), \"bert_sentimentAnaly\")"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 5 ========\n",
            "Training...\n",
            "  Batch   100  of  2,500.    Elapsed: 0:01:03.      Current Loss: 0.6109544038772583\n",
            "  Batch   200  of  2,500.    Elapsed: 0:02:06.      Current Loss: 0.5833436846733093\n",
            "  Batch   300  of  2,500.    Elapsed: 0:03:09.      Current Loss: 0.05922584608197212\n",
            "  Batch   400  of  2,500.    Elapsed: 0:04:12.      Current Loss: 0.2766300141811371\n",
            "  Batch   500  of  2,500.    Elapsed: 0:05:15.      Current Loss: 0.4598260819911957\n",
            "  Batch   600  of  2,500.    Elapsed: 0:06:18.      Current Loss: 0.4994557797908783\n",
            "  Batch   700  of  2,500.    Elapsed: 0:07:21.      Current Loss: 0.033208929002285004\n",
            "  Batch   800  of  2,500.    Elapsed: 0:08:24.      Current Loss: 0.29058030247688293\n",
            "  Batch   900  of  2,500.    Elapsed: 0:09:27.      Current Loss: 0.1628505438566208\n",
            "  Batch 1,000  of  2,500.    Elapsed: 0:10:29.      Current Loss: 0.3567003011703491\n",
            "  Batch 1,100  of  2,500.    Elapsed: 0:11:32.      Current Loss: 0.3144363462924957\n",
            "  Batch 1,200  of  2,500.    Elapsed: 0:12:35.      Current Loss: 0.4770054817199707\n",
            "  Batch 1,300  of  2,500.    Elapsed: 0:13:38.      Current Loss: 0.3673704266548157\n",
            "  Batch 1,400  of  2,500.    Elapsed: 0:14:41.      Current Loss: 0.12227920442819595\n",
            "  Batch 1,500  of  2,500.    Elapsed: 0:15:44.      Current Loss: 0.7744752764701843\n",
            "  Batch 1,600  of  2,500.    Elapsed: 0:16:47.      Current Loss: 0.16373410820960999\n",
            "  Batch 1,700  of  2,500.    Elapsed: 0:17:50.      Current Loss: 0.3470095694065094\n",
            "  Batch 1,800  of  2,500.    Elapsed: 0:18:53.      Current Loss: 0.09214652329683304\n",
            "  Batch 1,900  of  2,500.    Elapsed: 0:19:56.      Current Loss: 0.057430949062108994\n",
            "  Batch 2,000  of  2,500.    Elapsed: 0:20:59.      Current Loss: 0.22810278832912445\n",
            "  Batch 2,100  of  2,500.    Elapsed: 0:22:02.      Current Loss: 0.03491725027561188\n",
            "  Batch 2,200  of  2,500.    Elapsed: 0:23:05.      Current Loss: 0.2704513967037201\n",
            "  Batch 2,300  of  2,500.    Elapsed: 0:24:08.      Current Loss: 0.1741001456975937\n",
            "  Batch 2,400  of  2,500.    Elapsed: 0:25:11.      Current Loss: 0.2762575149536133\n",
            "\n",
            "  Average training loss: 0.28\n",
            "  Training epcoh took: 0:26:14\n",
            "\n",
            "\n",
            "\n",
            "======== Epoch 2 / 5 ========\n",
            "Training...\n",
            "  Batch   100  of  2,500.    Elapsed: 0:01:03.      Current Loss: 0.022847933694720268\n",
            "  Batch   200  of  2,500.    Elapsed: 0:02:06.      Current Loss: 0.02156086638569832\n",
            "  Batch   300  of  2,500.    Elapsed: 0:03:09.      Current Loss: 0.6586505770683289\n",
            "  Batch   400  of  2,500.    Elapsed: 0:04:13.      Current Loss: 0.11829950660467148\n",
            "  Batch   500  of  2,500.    Elapsed: 0:05:15.      Current Loss: 0.11409907788038254\n",
            "  Batch   600  of  2,500.    Elapsed: 0:06:18.      Current Loss: 0.3882422149181366\n",
            "  Batch   700  of  2,500.    Elapsed: 0:07:21.      Current Loss: 0.4091350734233856\n",
            "  Batch   800  of  2,500.    Elapsed: 0:08:24.      Current Loss: 0.482103168964386\n",
            "  Batch   900  of  2,500.    Elapsed: 0:09:27.      Current Loss: 0.10724196583032608\n",
            "  Batch 1,000  of  2,500.    Elapsed: 0:10:30.      Current Loss: 0.013151571154594421\n",
            "  Batch 1,100  of  2,500.    Elapsed: 0:11:34.      Current Loss: 0.5295392274856567\n",
            "  Batch 1,200  of  2,500.    Elapsed: 0:12:37.      Current Loss: 0.21533922851085663\n",
            "  Batch 1,300  of  2,500.    Elapsed: 0:13:40.      Current Loss: 0.1464383453130722\n",
            "  Batch 1,400  of  2,500.    Elapsed: 0:14:43.      Current Loss: 0.34783560037612915\n",
            "  Batch 1,500  of  2,500.    Elapsed: 0:15:45.      Current Loss: 0.0744837075471878\n",
            "  Batch 1,600  of  2,500.    Elapsed: 0:16:48.      Current Loss: 0.21034756302833557\n",
            "  Batch 1,700  of  2,500.    Elapsed: 0:17:51.      Current Loss: 0.09269242733716965\n",
            "  Batch 1,800  of  2,500.    Elapsed: 0:18:54.      Current Loss: 0.014809730462729931\n",
            "  Batch 1,900  of  2,500.    Elapsed: 0:19:57.      Current Loss: 0.009326483123004436\n",
            "  Batch 2,000  of  2,500.    Elapsed: 0:21:00.      Current Loss: 0.01725894771516323\n",
            "  Batch 2,100  of  2,500.    Elapsed: 0:22:03.      Current Loss: 0.034134287387132645\n",
            "  Batch 2,200  of  2,500.    Elapsed: 0:23:06.      Current Loss: 0.02035965956747532\n",
            "  Batch 2,300  of  2,500.    Elapsed: 0:24:09.      Current Loss: 0.039721693843603134\n",
            "  Batch 2,400  of  2,500.    Elapsed: 0:25:12.      Current Loss: 0.09901370853185654\n",
            "\n",
            "  Average training loss: 0.15\n",
            "  Training epcoh took: 0:26:15\n",
            "\n",
            "\n",
            "\n",
            "======== Epoch 3 / 5 ========\n",
            "Training...\n",
            "  Batch   100  of  2,500.    Elapsed: 0:01:03.      Current Loss: 0.009886547923088074\n",
            "  Batch   200  of  2,500.    Elapsed: 0:02:06.      Current Loss: 0.008653075434267521\n",
            "  Batch   300  of  2,500.    Elapsed: 0:03:09.      Current Loss: 0.0456257164478302\n",
            "  Batch   400  of  2,500.    Elapsed: 0:04:13.      Current Loss: 0.010429747402668\n",
            "  Batch   500  of  2,500.    Elapsed: 0:05:15.      Current Loss: 0.0027077931445091963\n",
            "  Batch   600  of  2,500.    Elapsed: 0:06:18.      Current Loss: 0.14886920154094696\n",
            "  Batch   700  of  2,500.    Elapsed: 0:07:21.      Current Loss: 0.0049066743813455105\n",
            "  Batch   800  of  2,500.    Elapsed: 0:08:24.      Current Loss: 0.007567611522972584\n",
            "  Batch   900  of  2,500.    Elapsed: 0:09:28.      Current Loss: 0.028144875541329384\n",
            "  Batch 1,000  of  2,500.    Elapsed: 0:10:31.      Current Loss: 0.36676713824272156\n",
            "  Batch 1,100  of  2,500.    Elapsed: 0:11:34.      Current Loss: 0.07594761997461319\n",
            "  Batch 1,200  of  2,500.    Elapsed: 0:12:37.      Current Loss: 0.003887501312419772\n",
            "  Batch 1,300  of  2,500.    Elapsed: 0:13:40.      Current Loss: 0.10041393339633942\n",
            "  Batch 1,400  of  2,500.    Elapsed: 0:14:43.      Current Loss: 0.019407542422413826\n",
            "  Batch 1,500  of  2,500.    Elapsed: 0:15:46.      Current Loss: 0.019945872947573662\n",
            "  Batch 1,600  of  2,500.    Elapsed: 0:16:49.      Current Loss: 0.007672522217035294\n",
            "  Batch 1,700  of  2,500.    Elapsed: 0:17:52.      Current Loss: 0.057168640196323395\n",
            "  Batch 1,800  of  2,500.    Elapsed: 0:18:54.      Current Loss: 0.07722651958465576\n",
            "  Batch 1,900  of  2,500.    Elapsed: 0:19:57.      Current Loss: 0.14089880883693695\n",
            "  Batch 2,000  of  2,500.    Elapsed: 0:21:00.      Current Loss: 0.01763509027659893\n",
            "  Batch 2,100  of  2,500.    Elapsed: 0:22:03.      Current Loss: 0.00355517934076488\n",
            "  Batch 2,200  of  2,500.    Elapsed: 0:23:05.      Current Loss: 0.007788229268044233\n",
            "  Batch 2,300  of  2,500.    Elapsed: 0:24:08.      Current Loss: 0.24499908089637756\n",
            "  Batch 2,400  of  2,500.    Elapsed: 0:25:11.      Current Loss: 0.009409399703145027\n",
            "\n",
            "  Average training loss: 0.09\n",
            "  Training epcoh took: 0:26:14\n",
            "\n",
            "\n",
            "\n",
            "======== Epoch 4 / 5 ========\n",
            "Training...\n",
            "  Batch   100  of  2,500.    Elapsed: 0:01:03.      Current Loss: 0.002019834239035845\n",
            "  Batch   200  of  2,500.    Elapsed: 0:02:06.      Current Loss: 0.004473638255149126\n",
            "  Batch   300  of  2,500.    Elapsed: 0:03:09.      Current Loss: 0.010391666553914547\n",
            "  Batch   400  of  2,500.    Elapsed: 0:04:12.      Current Loss: 0.6777495741844177\n",
            "  Batch   500  of  2,500.    Elapsed: 0:05:15.      Current Loss: 0.1652369350194931\n",
            "  Batch   600  of  2,500.    Elapsed: 0:06:18.      Current Loss: 0.061150968074798584\n",
            "  Batch   700  of  2,500.    Elapsed: 0:07:21.      Current Loss: 0.006554014515131712\n",
            "  Batch   800  of  2,500.    Elapsed: 0:08:24.      Current Loss: 0.017909228801727295\n",
            "  Batch   900  of  2,500.    Elapsed: 0:09:27.      Current Loss: 0.017538750544190407\n",
            "  Batch 1,000  of  2,500.    Elapsed: 0:10:30.      Current Loss: 0.005544168874621391\n",
            "  Batch 1,100  of  2,500.    Elapsed: 0:11:33.      Current Loss: 0.06850127130746841\n",
            "  Batch 1,200  of  2,500.    Elapsed: 0:12:36.      Current Loss: 0.11932163685560226\n",
            "  Batch 1,300  of  2,500.    Elapsed: 0:13:38.      Current Loss: 0.010183217003941536\n",
            "  Batch 1,400  of  2,500.    Elapsed: 0:14:41.      Current Loss: 0.3822420835494995\n",
            "  Batch 1,500  of  2,500.    Elapsed: 0:15:44.      Current Loss: 0.002225620439276099\n",
            "  Batch 1,600  of  2,500.    Elapsed: 0:16:47.      Current Loss: 0.028931040316820145\n",
            "  Batch 1,700  of  2,500.    Elapsed: 0:17:49.      Current Loss: 0.010608446784317493\n",
            "  Batch 1,800  of  2,500.    Elapsed: 0:18:52.      Current Loss: 0.005061644595116377\n",
            "  Batch 1,900  of  2,500.    Elapsed: 0:19:55.      Current Loss: 0.005534956697374582\n",
            "  Batch 2,000  of  2,500.    Elapsed: 0:20:58.      Current Loss: 0.1438150703907013\n",
            "  Batch 2,100  of  2,500.    Elapsed: 0:22:00.      Current Loss: 0.3978665769100189\n",
            "  Batch 2,200  of  2,500.    Elapsed: 0:23:03.      Current Loss: 0.004420069511979818\n",
            "  Batch 2,300  of  2,500.    Elapsed: 0:24:06.      Current Loss: 0.0023049917072057724\n",
            "  Batch 2,400  of  2,500.    Elapsed: 0:25:09.      Current Loss: 0.004490564577281475\n",
            "\n",
            "  Average training loss: 0.06\n",
            "  Training epcoh took: 0:26:12\n",
            "\n",
            "\n",
            "\n",
            "======== Epoch 5 / 5 ========\n",
            "Training...\n",
            "  Batch   100  of  2,500.    Elapsed: 0:01:03.      Current Loss: 0.029116908088326454\n",
            "  Batch   200  of  2,500.    Elapsed: 0:02:06.      Current Loss: 0.002241367008537054\n",
            "  Batch   300  of  2,500.    Elapsed: 0:03:09.      Current Loss: 0.027148103341460228\n",
            "  Batch   400  of  2,500.    Elapsed: 0:04:11.      Current Loss: 0.004725874867290258\n",
            "  Batch   500  of  2,500.    Elapsed: 0:05:14.      Current Loss: 0.021904051303863525\n",
            "  Batch   600  of  2,500.    Elapsed: 0:06:17.      Current Loss: 0.0010167770087718964\n",
            "  Batch   700  of  2,500.    Elapsed: 0:07:20.      Current Loss: 0.01152398344129324\n",
            "  Batch   800  of  2,500.    Elapsed: 0:08:23.      Current Loss: 0.004674998112022877\n",
            "  Batch   900  of  2,500.    Elapsed: 0:09:26.      Current Loss: 0.25517556071281433\n",
            "  Batch 1,000  of  2,500.    Elapsed: 0:10:29.      Current Loss: 0.004383913241326809\n",
            "  Batch 1,100  of  2,500.    Elapsed: 0:11:32.      Current Loss: 0.02914592996239662\n",
            "  Batch 1,200  of  2,500.    Elapsed: 0:12:35.      Current Loss: 0.0020067603327333927\n",
            "  Batch 1,300  of  2,500.    Elapsed: 0:13:38.      Current Loss: 0.0028645717538893223\n",
            "  Batch 1,400  of  2,500.    Elapsed: 0:14:41.      Current Loss: 0.004990463610738516\n",
            "  Batch 1,500  of  2,500.    Elapsed: 0:15:44.      Current Loss: 0.004184377379715443\n",
            "  Batch 1,600  of  2,500.    Elapsed: 0:16:47.      Current Loss: 0.0010606498690322042\n",
            "  Batch 1,700  of  2,500.    Elapsed: 0:17:50.      Current Loss: 0.030494004487991333\n",
            "  Batch 1,800  of  2,500.    Elapsed: 0:18:53.      Current Loss: 0.0756428986787796\n",
            "  Batch 1,900  of  2,500.    Elapsed: 0:19:56.      Current Loss: 0.04414118453860283\n",
            "  Batch 2,000  of  2,500.    Elapsed: 0:20:59.      Current Loss: 0.0040814317762851715\n",
            "  Batch 2,100  of  2,500.    Elapsed: 0:22:02.      Current Loss: 0.022977115586400032\n",
            "  Batch 2,200  of  2,500.    Elapsed: 0:23:05.      Current Loss: 0.14830505847930908\n",
            "  Batch 2,300  of  2,500.    Elapsed: 0:24:08.      Current Loss: 0.15159542858600616\n",
            "  Batch 2,400  of  2,500.    Elapsed: 0:25:11.      Current Loss: 0.0851266086101532\n",
            "\n",
            "  Average training loss: 0.06\n",
            "  Training epcoh took: 0:26:14\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZYSVvFNQm4r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "3655b0a6-5b56-40bb-de5f-18bfa56ad4e4"
      },
      "source": [
        "predictions, targets = computeScores(testing_loader,'Testing set')"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing set\n",
            "  Accuracy Score = 0.99484\n",
            "  F1 Score (Micro) = 0.99484\n",
            "  F1 Score (Macro) = 0.9948399431237891\n",
            " \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EbjbSHtTmsz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c88bdb10-2cce-482d-d79a-4c046e72a396"
      },
      "source": [
        "roc_auc_score(targets, predictions)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.99484"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fjsyvalTLyK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "d65879bd-de82-482f-b040-0d5d3da28f83"
      },
      "source": [
        "# classification_report(y_true, y_pred)\n",
        "print(classification_report(targets, predictions))"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      0.99      0.99     12500\n",
            "         1.0       0.99      1.00      0.99     12500\n",
            "\n",
            "    accuracy                           0.99     25000\n",
            "   macro avg       0.99      0.99      0.99     25000\n",
            "weighted avg       0.99      0.99      0.99     25000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}