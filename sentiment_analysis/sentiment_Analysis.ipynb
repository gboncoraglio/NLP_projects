{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentiment_Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "momUpfyaTt5s",
        "colab_type": "text"
      },
      "source": [
        "# Sentiment analysis \n",
        "\n",
        "This project involves building a movie review classifier\n",
        "Sentiment analysis is the task of classifying the polarity of a given text.\n",
        "\n",
        "The dataset used is the IMDb dataset. The data was compiled by Andrew Maas (http://ai.stanford.edu/~amaas/data/sentiment/)\n",
        "This dataset contains 50,000 reviews labelled as either positive or negative. Moreover, this datset is balanced, i.e. equal number of postive and negative reviews. Moreover, for each movie, a number equal or less than 30 reviews is considered since reviews tend to correlate.\n",
        "Finally, only reviews with either a low score (less or equal to 4 out of 10) or high score (greater or equal to 7 out of 10) are considered to avoid considering reviews that are neutral.\n",
        "For the purpose of testing, we are going to use only the training data since they are labelled. \n",
        "\n",
        "\n",
        "For this problem, we are going to use a state-of-the-art language model pre-training model, BERT (Bidirectional Encoder Representations from Transformers)model and we will apply transfer learning to adapt the model to the sentiment analysis problem. \n",
        "The model is going to be loaded from the library transformers (https://github.com/huggingface/transformers). We are going to use Pytorch as framework for this problem.\n",
        "\n",
        "## Data processing \n",
        "For the data processing task:\n",
        "- Lowercasing is applied. Lowercasing ALL the text data, is one of the simplest and most effective form of text preprocessing. Some pre-trained model might give a different answers if the input is capitalized or not. Of course, in some specific cases, it might be useful not to change to lowercasing.\n",
        "- Noise Removal is applied. Noise removal is about removing characters digits and pieces of text that can interfere with your text analysis. For instances, punctuations signs, parenthesis, mathematical symbols are removed (for instance, '.' '(' '?' or '<' or ':' are removed).\n",
        "- Since we are using BERT model, we keep the words as they are, i.e. stemming is **not** used. Stemming is the process of reducing inflection in words (e.g. connected, connections) to their root form (e.g. connect).\n",
        "- For the same reason explained before, lemmatization is **not** used. Lemmatization is very similar to stemming. The difference is that, lemmatization doesn’t just chop things off, it actually transforms words to the actual root.\n",
        "\n",
        "Once the data is pre-processed, then the training data (25,000 samples) are used to train the model and testing data (25,000 samples) are used to test the final model.\n",
        "\n",
        "## Building the model\n",
        "The model used is the pre-trained BERT model with dropout and a final fully connected layer. \n",
        "Specifically, it is used BERT base model – with a hidden size of 768, with 12 layers (transformer blocks), 12 attention heads, and 110 million parameters.\n",
        "Droput is added for the purpose of Regulariaztion and the linear layer for Classification. The dropout probability is always kept at 0.1\n",
        "\n",
        "The input to BERT is a sequence of words, and the output is a sequence of vectors.\n",
        "BERT model can accept up to 512 words. However, some review sentences might be longer than this number. In order to deal with this, we apply the strategy described in the paper \"How to Fine-Tune BERT for Text Classification?\" by Sun et al. They propose to use a truncation method, and select part of the beginning and part of the end of the sentence. \n",
        "In fact, the key information of an article is at the beginning and end. Thus, empirically the first 128 and the last 382 tokens are selected if longer than 512.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Training the model\n",
        "The final layer is used to compute the loss and determine the accuracy of the model. The loss used is Binary Cross Entropy which is implemented as BCELogits Loss in PyTorch.\n",
        "A learning rate of 2.5e-5 is used and the warm-up proportion is 0.1. The optimization algorithm used is ADAM with $\\beta_1 = 0.9$ and $\\beta_2 = 0.999$. The batch size is 12 and the number of epochs is 4. \n",
        "\n",
        "## Testing the model\n",
        "Once the model is trained, the algorithm is tested on the test set and reach an accuracy of $\\approx 93\\%$ using only 4 epochs! this shows how transfer learning is important for using these models for additional tasks!\n",
        "\n",
        "\n",
        "Thanks,\n",
        "Gabriele Boncoraglio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bUTE_TJXvc0",
        "colab_type": "text"
      },
      "source": [
        "Import some useful libraries for running the code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Yfj641sF-rO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -Iv transformers==2.2.2\n",
        "!pip install pytorch_pretrained_bert==0.4.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YOxX0KG3cKs",
        "colab_type": "code",
        "outputId": "d7009710-2287-4a98-aedd-3d663768b907",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import re,os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score, classification_report\n",
        "from sklearn import metrics\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from contextlib import contextmanager\n",
        "import time, random \n",
        "\n",
        "import transformers\n",
        "from transformers import BertTokenizer, BertModel, BertConfig\n",
        "from pytorch_pretrained_bert.optimization import BertAdam, warmup_linear\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "print(device)\n",
        "\n",
        "os.chdir('/content/drive/My Drive/Colab Notebooks/Sentiment Analysis')\n",
        "#True if you already trained the model\n",
        "trained = False"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n",
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dR-KHZltegU_",
        "colab_type": "text"
      },
      "source": [
        "## Data processing "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7XWOJ-4X1j3",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pR3a1Iht5Iu7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = []\n",
        "for line in open('training_dataset.txt', 'r'):\n",
        "    train.append(line.strip())\n",
        "\n",
        "test = []\n",
        "for line in open('testing_dataset.txt', 'r'):\n",
        "    test.append(line.strip())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUEj0eas46s0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data noise removal\n",
        "def preprocess_reviews(reviews):\n",
        "    reviews = [re.compile(\"[.;:!\\'?,\\\"()\\[\\]]\").sub(\"\", line.lower()) for line in reviews]\n",
        "    reviews = [re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
        ".sub(\" \", line) for line in reviews]\n",
        "\n",
        "    return reviews\n",
        "\n",
        "train1 = preprocess_reviews(train)\n",
        "test1 = preprocess_reviews(test)\n",
        "#Creating labels for dataset\n",
        "#first 12500 are postive and the rest is negative\n",
        "target = [1 if i < 12500 else 0 for i in range(25000)]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npMyH0-kgOCd",
        "colab_type": "code",
        "outputId": "1fe2c8ba-fe46-43bc-b58a-e0db69ccbcbc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# Dealing with long sentences\n",
        "# Looking average and max length of the training sentences dataset\n",
        "maxLen = 0\n",
        "avgLen = 0\n",
        "for stri in train1:\n",
        "  s = stri.split()\n",
        "  maxLen = max(maxLen,len(s))\n",
        "  avgLen += len(s)\n",
        "avgLen /= len(train1)\n",
        "print('(training) average is: ',round(avgLen))\n",
        "print('(training) max length is: ',maxLen)\n",
        "\n",
        "\n",
        "maxLen = 0\n",
        "avgLen = 0\n",
        "for stri in test1:\n",
        "  s = stri.split()\n",
        "  maxLen = max(maxLen,len(s))\n",
        "  avgLen += len(s)\n",
        "avgLen /= len(train1)\n",
        "print('(testing) average is: ',round(avgLen))\n",
        "print('(testing) max length is: ',maxLen)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(training) average is:  233\n",
            "(training) max length is:  2473\n",
            "(testing) average is:  228\n",
            "(testing) max length is:  2245\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEn1_rBW5WHv",
        "colab_type": "code",
        "outputId": "dae43a8c-6aca-4cac-9bd1-45d3d879d2aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# Visualizing some of the comments and the response to see our data\n",
        "d_train = {'comment_text': train1, 'list': target}\n",
        "d_test = {'comment_text': test1, 'list': target}\n",
        "\n",
        "df_train = pd.DataFrame(data=d_train)\n",
        "df_test = pd.DataFrame(data=d_test)\n",
        "\n",
        "df_train.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>comment_text</th>\n",
              "      <th>list</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>bromwell high is a cartoon comedy it ran at th...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>homelessness or houselessness as george carlin...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>brilliant over acting by lesley ann warren bes...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>this is easily the most underrated film inn th...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>this is not the typical mel brooks film it was...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                        comment_text  list\n",
              "0  bromwell high is a cartoon comedy it ran at th...     1\n",
              "1  homelessness or houselessness as george carlin...     1\n",
              "2  brilliant over acting by lesley ann warren bes...     1\n",
              "3  this is easily the most underrated film inn th...     1\n",
              "4  this is not the typical mel brooks film it was...     1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwQSaks2lsET",
        "colab_type": "code",
        "outputId": "77fb2477-ba4a-4185-e5f9-52d90cf2a614",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "\n",
        "# Visualizing the dataset size\n",
        "print(\"FULL Dataset: {}\".format(df_train.shape[0]+df_test.shape[0]))\n",
        "print(\"TRAIN Dataset: {}\".format(df_train.shape))\n",
        "print(\"TEST Dataset: {}\".format(df_test.shape))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FULL Dataset: 50000\n",
            "TRAIN Dataset: (25000, 2)\n",
            "TEST Dataset: (25000, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgInAM1QhxT-",
        "colab_type": "text"
      },
      "source": [
        "## Building the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGBInpSnwHA9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# nice way to report running times\n",
        "@contextmanager\n",
        "def timer(name):\n",
        "    t0 = time.time()\n",
        "    yield\n",
        "    print(f'[{name}] done in {time.time() - t0:.0f} s')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9aOhBZdW52gs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Sections of config\n",
        "# Defining some key variables that will be used later on in the training\n",
        "MAX_LEN = 512\n",
        "TRAIN_BATCH_SIZE = 12\n",
        "TEST_BATCH_SIZE = 12\n",
        "EPOCHS = 4\n",
        "LEARNING_RATE = 2e-05\n",
        "SIZE_TRAIN = df_train.shape[0]\n",
        "\n",
        "ACCUM_STEPS = 1          # wait for several backward steps, then one optimization step\n",
        "WARMUP = 0.1             # warmup helps to tackle instability in the initial phase of training\n",
        "USE_APEX = True         \n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qcp7Rr7llV8D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8dc0bf57-fa4a-40a4-b571-ec6b058546d9"
      },
      "source": [
        "if USE_APEX:\n",
        "    with timer('install Nvidia apex'):\n",
        "        # Installing Nvidia Apex\n",
        "        os.system('git clone https://github.com/NVIDIA/apex; cd apex; pip install -v --no-cache-dir' + \n",
        "                  ' --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./')\n",
        "        os.system('rm -rf apex/.git') \n",
        "        from apex import amp\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[install Nvidia apex] done in 520 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbwELROF_EXZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "PyTorch supports two different types of datasets:\n",
        "- map-style datasets,\n",
        "- iterable-style datasets.\n",
        "\n",
        "        - Map-style datasets\n",
        "        A map-style dataset is one that implements the __getitem__() and __len__() protocols, \n",
        "        and represents a map from (possibly non-integral) indices/keys to data samples.\n",
        "        For example, such a dataset, when accessed with dataset[idx], \n",
        "        could read the idx-th image and its corresponding label from a folder on the disk.\n",
        "        \n",
        "        - An iterable-style dataset is an instance of a subclass of IterableDataset that \n",
        "        implements the __iter__() protocol, and represents an iterable over data samples. \n",
        "        This type of datasets is particularly suitable for cases where random reads \n",
        "        are expensive or even improbable, and where the batch size depends on the fetched data.\n",
        "        For example, such a dataset, when called iter(dataset), \n",
        "        could return a stream of data reading from a database, a remote server, \n",
        "        or even logs generated in real time.\n",
        "'''\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "\n",
        "    def __init__(self, dataframe, tokenizer, max_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = dataframe\n",
        "        self.comment_text = dataframe.comment_text\n",
        "        self.targets = self.data.list\n",
        "        self.max_len = max_len\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.comment_text)\n",
        "    \n",
        " \n",
        "    def __getitem__(self, index):\n",
        "        comment_text = str(self.comment_text[index])\n",
        "        comment_text = \" \".join(comment_text.split())\n",
        "        if len(comment_text) > 512:\n",
        "          comment_text = comment_text[:128] + comment_text[381:] \n",
        "\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            comment_text,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            stride = 0,\n",
        "            truncation_strategy = 'longest_first',\n",
        "            pad_to_max_length = True,\n",
        "            return_token_type_ids=True\n",
        "        )\n",
        "        ids = inputs['input_ids']\n",
        "        mask = inputs['attention_mask']\n",
        "        token_type_ids = inputs[\"token_type_ids\"]\n",
        "\n",
        "\n",
        "        return {\n",
        "            'ids': torch.tensor(ids, dtype=torch.long),\n",
        "            'mask': torch.tensor(mask, dtype=torch.long),\n",
        "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
        "            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n",
        "        }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOKqHZBpCa52",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "\n",
        "\n",
        "test_params = {'batch_size': TEST_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "               }\n",
        "\n",
        "# Creating a Map-style datasets for PyTorch\n",
        "training_set = CustomDataset(df_train, tokenizer, MAX_LEN)\n",
        "testing_set = CustomDataset(df_test, tokenizer, MAX_LEN)\n",
        "\n",
        "training_loader = DataLoader(training_set, **train_params)\n",
        "testing_loader = DataLoader(testing_set, **test_params)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A697cQy_CiDY",
        "colab_type": "code",
        "outputId": "f50a9b22-4328-449e-c3ae-7b0f5310f0e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "class BERTClass(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        '''\n",
        "            The bert model output two quantities.\n",
        "            The second output, output_1, is the pooled output and it is passed \n",
        "            to the Drop Out layer and \n",
        "            the subsequent output is given to the Linear layer.\n",
        "        '''\n",
        "        super(BERTClass, self).__init__()\n",
        "        self.l1 = transformers.BertModel.from_pretrained('bert-base-uncased')\n",
        "        # #Freeze bert layers\n",
        "        # for p in self.l1.parameters():\n",
        "        #     p.requires_grad = False\n",
        "        self.l2 = torch.nn.Dropout(0.1)\n",
        "        self.l3 = torch.nn.Linear(768, 1,bias=True)\n",
        "    \n",
        "    def forward(self, ids, mask, token_type_ids):\n",
        "        _, output_1= self.l1(ids, attention_mask = mask, token_type_ids = token_type_ids)\n",
        "        output_2 = self.l2(output_1)\n",
        "        output = self.l3(output_2)\n",
        "        return output\n",
        "\n",
        "model = BERTClass()\n",
        "\n",
        "if trained:\n",
        "  model.load_state_dict(torch.load(\"bert_sentimentAnaly1\"))\n",
        "model.to(device)\n",
        "\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BERTClass(\n",
              "  (l1): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (l2): Dropout(p=0.1, inplace=False)\n",
              "  (l3): Linear(in_features=768, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBz_nrOAnHJS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# make results fully reproducible\n",
        "def seed_everything(seed=123):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhQDRwXUFuNx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "d7d3a348-7e7c-48fb-ee69-0a16c24c843f"
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def loss_fn(outputs, targets):\n",
        "    return torch.nn.BCEWithLogitsLoss()(outputs, targets)\n",
        "\n",
        "\n",
        "        \n",
        "if USE_APEX:\n",
        "    num_train_optimization_steps = int(EPOCHS * SIZE_TRAIN / TRAIN_BATCH_SIZE / ACCUM_STEPS)\n",
        "    optimizer = BertAdam(model.parameters(),\n",
        "                          lr=LEARNING_RATE, warmup=WARMUP,\n",
        "                          t_total=num_train_optimization_steps)\n",
        "\n",
        "    model, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\",verbosity=1)\n",
        "    model = model.train()\n",
        "else:\n",
        "    # All the parameters are being trained\n",
        "    optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE, weight_decay = 5e-4)\n",
        "\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O1\n",
            "cast_model_type        : None\n",
            "patch_torch_functions  : True\n",
            "keep_batchnorm_fp32    : None\n",
            "master_weights         : None\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O1\n",
            "cast_model_type        : None\n",
            "patch_torch_functions  : True\n",
            "keep_batchnorm_fp32    : None\n",
            "master_weights         : None\n",
            "loss_scale             : dynamic\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdipCvZkwnA1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n",
        "def validation(loader):\n",
        "    model.eval()\n",
        "    fin_targets=[]\n",
        "    fin_outputs=[]\n",
        "    with torch.no_grad():\n",
        "        for _, data in enumerate(loader, 0):\n",
        "            ids = data['ids'].to(device, dtype = torch.long)\n",
        "            mask = data['mask'].to(device, dtype = torch.long)\n",
        "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "            targets = data['targets'].to(device, dtype = torch.float)\n",
        "            targets = targets.unsqueeze(1)\n",
        "            outputs = model(ids, mask, token_type_ids)\n",
        "            fin_targets.extend(targets.cpu().detach().numpy().tolist())\n",
        "            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
        "    return fin_outputs, fin_targets\n",
        "\n",
        "def computeScores(testing_loader,typeSet):\n",
        "    outputs, targets = validation(testing_loader) \n",
        "    outputs = np.array(outputs) >= 0.5\n",
        "    accuracy = metrics.accuracy_score(targets, outputs)\n",
        "    f1_score_micro = metrics.f1_score(targets, outputs, average='micro')\n",
        "    f1_score_macro = metrics.f1_score(targets, outputs, average='macro')\n",
        "    print(\"{}\".format(typeSet))\n",
        "    print(f\"  Accuracy Score = {accuracy}\")\n",
        "    print(f\"  F1 Score (Micro) = {f1_score_micro}\")\n",
        "    print(f\"  F1 Score (Macro) = {f1_score_macro}\")\n",
        "    print(\" \")\n",
        "    return outputs, targets\n",
        "    # return accuracy,f1_score_micro,f1_score_macro"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqDk05FgEkTs",
        "colab_type": "code",
        "outputId": "7e58e43d-7851-4c72-d9ba-3376af4e150f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "seed_everything(seed=123)\n",
        "if not trained:\n",
        "  for epoch in range(EPOCHS):\n",
        "      # ========================================\n",
        "      #               Training\n",
        "      # ========================================\n",
        "      \n",
        "      # Perform one full pass over the training set.\n",
        "\n",
        "      print(\"\")\n",
        "      print('======== Epoch {:} / {:} ========'.format(epoch + 1, EPOCHS))\n",
        "      print('Training...')\n",
        "      # Measure how long the training epoch takes.\n",
        "      t0 = time.time()\n",
        "\n",
        "      # Reset the total loss for this epoch.\n",
        "      total_train_loss = 0\n",
        "\n",
        "      # Put the model into training mode.\n",
        "      model.train()\n",
        "\n",
        "      # for _,data in enumerate(training_loader, 0):\n",
        "      for step, data in enumerate(training_loader):\n",
        "\n",
        "          ids = data['ids'].to(device, dtype = torch.long)\n",
        "          mask = data['mask'].to(device, dtype = torch.long)\n",
        "          token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "          targets = data['targets'].to(device, dtype = torch.float)\n",
        "          targets = targets.unsqueeze(1)\n",
        "          # target = target.float()\n",
        "          outputs = model(ids, mask, token_type_ids)\n",
        "\n",
        "\n",
        "          # We need to clear any previously calculated gradients before performing a\n",
        "          # backward pass.\n",
        "          optimizer.zero_grad()\n",
        "          loss = loss_fn(outputs, targets)\n",
        "\n",
        "          # Progress update every 100 batches.\n",
        "          if step % 100 == 0 and not step == 0:\n",
        "              # Calculate elapsed time in minutes.\n",
        "              elapsed = format_time(time.time() - t0)\n",
        "              \n",
        "              # Report progress.\n",
        "              print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.      Current Loss: {}'.format(step, len(training_loader), elapsed,loss.item()))\n",
        "\n",
        "      \n",
        "          total_train_loss += loss.item()\n",
        "\n",
        "          \n",
        "          optimizer.zero_grad()\n",
        "          if USE_APEX:\n",
        "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "                    scaled_loss.backward()\n",
        "          else:\n",
        "                loss.backward()\n",
        "\n",
        "          optimizer.step()\n",
        "\n",
        "      # Calculate the average loss over all of the batches.\n",
        "      avg_train_loss = total_train_loss / len(training_loader)            \n",
        "      \n",
        "      # Measure how long this epoch took.\n",
        "      training_time = format_time(time.time() - t0)\n",
        "\n",
        "      print(\"\")\n",
        "      print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "      print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "      print(\"\")\n",
        "      # computeScores(testing_loader,'Testing set')\n",
        "      # computeScores(validating_loader,'Validation set')\n",
        "      # computeScores(validating_loader,'Validation set')\n",
        "      print(\"\")\n",
        "\n",
        "if not trained:\n",
        "  torch.save(model.state_dict(), \"bert_sentimentAnaly\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "  Batch   100  of  2,084.    Elapsed: 0:01:26.      Current Loss: 0.4339156150817871\n",
            "  Batch   200  of  2,084.    Elapsed: 0:02:53.      Current Loss: 0.3533976674079895\n",
            "  Batch   300  of  2,084.    Elapsed: 0:04:19.      Current Loss: 0.07138269394636154\n",
            "  Batch   400  of  2,084.    Elapsed: 0:05:45.      Current Loss: 0.5555235147476196\n",
            "  Batch   500  of  2,084.    Elapsed: 0:07:11.      Current Loss: 0.3588411808013916\n",
            "  Batch   600  of  2,084.    Elapsed: 0:08:38.      Current Loss: 0.6481459736824036\n",
            "  Batch   700  of  2,084.    Elapsed: 0:10:04.      Current Loss: 0.39229482412338257\n",
            "  Batch   800  of  2,084.    Elapsed: 0:11:30.      Current Loss: 0.10386266559362411\n",
            "  Batch   900  of  2,084.    Elapsed: 0:12:56.      Current Loss: 0.08484383672475815\n",
            "  Batch 1,000  of  2,084.    Elapsed: 0:14:22.      Current Loss: 0.30722078680992126\n",
            "  Batch 1,100  of  2,084.    Elapsed: 0:15:48.      Current Loss: 0.3215128183364868\n",
            "  Batch 1,200  of  2,084.    Elapsed: 0:17:14.      Current Loss: 0.22009995579719543\n",
            "  Batch 1,300  of  2,084.    Elapsed: 0:18:41.      Current Loss: 0.534180223941803\n",
            "  Batch 1,400  of  2,084.    Elapsed: 0:20:07.      Current Loss: 0.15762768685817719\n",
            "  Batch 1,500  of  2,084.    Elapsed: 0:21:33.      Current Loss: 0.18425703048706055\n",
            "  Batch 1,600  of  2,084.    Elapsed: 0:22:59.      Current Loss: 0.07597111165523529\n",
            "  Batch 1,700  of  2,084.    Elapsed: 0:24:24.      Current Loss: 0.24006912112236023\n",
            "  Batch 1,800  of  2,084.    Elapsed: 0:25:50.      Current Loss: 0.060300931334495544\n",
            "  Batch 1,900  of  2,084.    Elapsed: 0:27:16.      Current Loss: 0.5095974206924438\n",
            "  Batch 2,000  of  2,084.    Elapsed: 0:28:41.      Current Loss: 0.03482164442539215\n",
            "\n",
            "  Average training loss: 0.29\n",
            "  Training epcoh took: 0:29:53\n",
            "\n",
            "\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Training...\n",
            "  Batch   100  of  2,084.    Elapsed: 0:01:26.      Current Loss: 0.06112942099571228\n",
            "  Batch   200  of  2,084.    Elapsed: 0:02:51.      Current Loss: 0.06951210647821426\n",
            "  Batch   300  of  2,084.    Elapsed: 0:04:17.      Current Loss: 0.030937258154153824\n",
            "  Batch   400  of  2,084.    Elapsed: 0:05:43.      Current Loss: 0.34830960631370544\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "  Batch   500  of  2,084.    Elapsed: 0:07:09.      Current Loss: 0.2230784296989441\n",
            "  Batch   600  of  2,084.    Elapsed: 0:08:34.      Current Loss: 0.028951819986104965\n",
            "  Batch   700  of  2,084.    Elapsed: 0:10:00.      Current Loss: 0.035113975405693054\n",
            "  Batch   800  of  2,084.    Elapsed: 0:11:26.      Current Loss: 0.07868101447820663\n",
            "  Batch   900  of  2,084.    Elapsed: 0:12:52.      Current Loss: 0.022035298869013786\n",
            "  Batch 1,000  of  2,084.    Elapsed: 0:14:18.      Current Loss: 0.035886023193597794\n",
            "  Batch 1,100  of  2,084.    Elapsed: 0:15:43.      Current Loss: 0.05239076539874077\n",
            "  Batch 1,200  of  2,084.    Elapsed: 0:17:09.      Current Loss: 0.02844047173857689\n",
            "  Batch 1,300  of  2,084.    Elapsed: 0:18:35.      Current Loss: 0.12761837244033813\n",
            "  Batch 1,400  of  2,084.    Elapsed: 0:20:00.      Current Loss: 0.1548602283000946\n",
            "  Batch 1,500  of  2,084.    Elapsed: 0:21:26.      Current Loss: 0.0469636470079422\n",
            "  Batch 1,600  of  2,084.    Elapsed: 0:22:52.      Current Loss: 0.05183164402842522\n",
            "  Batch 1,700  of  2,084.    Elapsed: 0:24:17.      Current Loss: 0.03262760862708092\n",
            "  Batch 1,800  of  2,084.    Elapsed: 0:25:43.      Current Loss: 0.5912340879440308\n",
            "  Batch 1,900  of  2,084.    Elapsed: 0:27:09.      Current Loss: 0.013646814972162247\n",
            "  Batch 2,000  of  2,084.    Elapsed: 0:28:34.      Current Loss: 0.02960808388888836\n",
            "\n",
            "  Average training loss: 0.13\n",
            "  Training epcoh took: 0:29:46\n",
            "\n",
            "\n",
            "\n",
            "======== Epoch 3 / 4 ========\n",
            "Training...\n",
            "  Batch   100  of  2,084.    Elapsed: 0:01:26.      Current Loss: 0.011844526045024395\n",
            "  Batch   200  of  2,084.    Elapsed: 0:02:51.      Current Loss: 0.009109936654567719\n",
            "  Batch   300  of  2,084.    Elapsed: 0:04:17.      Current Loss: 0.01804531179368496\n",
            "  Batch   400  of  2,084.    Elapsed: 0:05:42.      Current Loss: 0.01354331523180008\n",
            "  Batch   500  of  2,084.    Elapsed: 0:07:08.      Current Loss: 0.0021164948120713234\n",
            "  Batch   600  of  2,084.    Elapsed: 0:08:34.      Current Loss: 0.52843177318573\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "  Batch   700  of  2,084.    Elapsed: 0:09:59.      Current Loss: 0.16893130540847778\n",
            "  Batch   800  of  2,084.    Elapsed: 0:11:25.      Current Loss: 0.014323290437459946\n",
            "  Batch   900  of  2,084.    Elapsed: 0:12:51.      Current Loss: 0.0031795005779713392\n",
            "  Batch 1,000  of  2,084.    Elapsed: 0:14:17.      Current Loss: 0.05138544738292694\n",
            "  Batch 1,100  of  2,084.    Elapsed: 0:15:42.      Current Loss: 0.0022334775421768427\n",
            "  Batch 1,200  of  2,084.    Elapsed: 0:17:08.      Current Loss: 0.003494159784168005\n",
            "  Batch 1,300  of  2,084.    Elapsed: 0:18:34.      Current Loss: 0.03929099440574646\n",
            "  Batch 1,400  of  2,084.    Elapsed: 0:20:00.      Current Loss: 0.22239786386489868\n",
            "  Batch 1,500  of  2,084.    Elapsed: 0:21:26.      Current Loss: 0.001992139033973217\n",
            "  Batch 1,600  of  2,084.    Elapsed: 0:22:51.      Current Loss: 0.00709461560472846\n",
            "  Batch 1,700  of  2,084.    Elapsed: 0:24:17.      Current Loss: 0.003050566418096423\n",
            "  Batch 1,800  of  2,084.    Elapsed: 0:25:43.      Current Loss: 0.003450194373726845\n",
            "  Batch 1,900  of  2,084.    Elapsed: 0:27:08.      Current Loss: 0.009515736252069473\n",
            "  Batch 2,000  of  2,084.    Elapsed: 0:28:34.      Current Loss: 0.0069764903746545315\n",
            "\n",
            "  Average training loss: 0.05\n",
            "  Training epcoh took: 0:29:46\n",
            "\n",
            "\n",
            "\n",
            "======== Epoch 4 / 4 ========\n",
            "Training...\n",
            "  Batch   100  of  2,084.    Elapsed: 0:01:26.      Current Loss: 0.5627481937408447\n",
            "  Batch   200  of  2,084.    Elapsed: 0:02:52.      Current Loss: 0.002324196044355631\n",
            "  Batch   300  of  2,084.    Elapsed: 0:04:18.      Current Loss: 0.002037991303950548\n",
            "  Batch   400  of  2,084.    Elapsed: 0:05:44.      Current Loss: 0.0027301739901304245\n",
            "  Batch   500  of  2,084.    Elapsed: 0:07:10.      Current Loss: 0.0018778585363179445\n",
            "  Batch   600  of  2,084.    Elapsed: 0:08:36.      Current Loss: 0.003913800697773695\n",
            "  Batch   700  of  2,084.    Elapsed: 0:10:01.      Current Loss: 0.0034294012002646923\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "  Batch   800  of  2,084.    Elapsed: 0:11:27.      Current Loss: 0.0023937514051795006\n",
            "  Batch   900  of  2,084.    Elapsed: 0:12:52.      Current Loss: 0.0010196133516728878\n",
            "  Batch 1,000  of  2,084.    Elapsed: 0:14:18.      Current Loss: 0.002909873379394412\n",
            "  Batch 1,100  of  2,084.    Elapsed: 0:15:44.      Current Loss: 0.002000114880502224\n",
            "  Batch 1,200  of  2,084.    Elapsed: 0:17:09.      Current Loss: 0.001763869309797883\n",
            "  Batch 1,300  of  2,084.    Elapsed: 0:18:35.      Current Loss: 0.0035532615147531033\n",
            "  Batch 1,400  of  2,084.    Elapsed: 0:20:01.      Current Loss: 0.002084882464259863\n",
            "  Batch 1,500  of  2,084.    Elapsed: 0:21:26.      Current Loss: 0.0011916474904865026\n",
            "  Batch 1,600  of  2,084.    Elapsed: 0:22:52.      Current Loss: 0.0022878162562847137\n",
            "  Batch 1,700  of  2,084.    Elapsed: 0:24:17.      Current Loss: 0.004084971733391285\n",
            "  Batch 1,800  of  2,084.    Elapsed: 0:25:42.      Current Loss: 0.0015074418624863029\n",
            "  Batch 1,900  of  2,084.    Elapsed: 0:27:08.      Current Loss: 0.0012771682813763618\n",
            "  Batch 2,000  of  2,084.    Elapsed: 0:28:33.      Current Loss: 0.022572418674826622\n",
            "\n",
            "  Average training loss: 0.02\n",
            "  Training epcoh took: 0:29:44\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZYSVvFNQm4r",
        "colab_type": "code",
        "outputId": "4a813fca-a4ee-4625-e04f-9f060f195071",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "predictions, targets = computeScores(testing_loader,'Testing set')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing set\n",
            "  Accuracy Score = 0.93072\n",
            "  F1 Score (Micro) = 0.93072\n",
            "  F1 Score (Macro) = 0.9307180095561274\n",
            " \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EbjbSHtTmsz",
        "colab_type": "code",
        "outputId": "a011387d-1872-4c65-ff0a-3a1325062c7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "roc_auc_score(targets, predictions)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.93072"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fjsyvalTLyK",
        "colab_type": "code",
        "outputId": "f1a4f1d8-4b3d-4aff-ecaa-e4fdb00af9bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "# classification_report(y_true, y_pred)\n",
        "print(classification_report(targets, predictions))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.94      0.93      0.93     12500\n",
            "         1.0       0.93      0.94      0.93     12500\n",
            "\n",
            "    accuracy                           0.93     25000\n",
            "   macro avg       0.93      0.93      0.93     25000\n",
            "weighted avg       0.93      0.93      0.93     25000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}