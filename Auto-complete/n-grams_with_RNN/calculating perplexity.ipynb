{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with JAX numpy and calculating perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens_length=568 inputs_length=512 targets_length=114 noise_density=0.15 mean_noise_span_length=3.0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gbonco/anaconda3/lib/python3.7/site-packages/jax/lib/xla_bridge.py:130: UserWarning: No GPU/TPU found, falling back to CPU.\n",
      "  warnings.warn('No GPU/TPU found, falling back to CPU.')\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import trax\n",
    "import trax.fastmath.numpy as np\n",
    "\n",
    "# Setting random seeds\n",
    "trax.supervised.trainer_lib.init_random_number_generators(32)\n",
    "numpy.random.seed(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The regular numpy array looks like this:\n",
      "\n",
      " [[0.85888927 0.37271115 0.55512878 0.95565655 0.7366696  0.81620514\n",
      "  0.10108656 0.92848807 0.60910917 0.59655344]\n",
      " [0.09178413 0.34518624 0.66275252 0.44171349 0.55148779 0.70371249\n",
      "  0.58940123 0.04993276 0.56179184 0.76635847]\n",
      " [0.91090833 0.09290995 0.90252139 0.46096041 0.45201847 0.99942549\n",
      "  0.16242374 0.70937058 0.16062408 0.81077677]\n",
      " [0.03514717 0.53488673 0.16650012 0.30841038 0.04506241 0.23857613\n",
      "  0.67483453 0.78238275 0.69520163 0.32895445]\n",
      " [0.49403187 0.52412136 0.29854125 0.46310814 0.98478429 0.50113492\n",
      "  0.39807245 0.72790532 0.86333097 0.02616954]]\n",
      "\n",
      "It is of type: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "numpy_array = numpy.random.random((5,10))\n",
    "print(f\"The regular numpy array looks like this:\\n\\n {numpy_array}\\n\")\n",
    "print(f\"It is of type: {type(numpy_array)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can easily cast regular numpy arrays or lists into trax numpy arrays using the `trax.fastmath.numpy.array()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The trax numpy array looks like this:\n",
      "\n",
      " [[0.8588893  0.37271115 0.55512875 0.9556565  0.7366696  0.81620514\n",
      "  0.10108656 0.9284881  0.60910916 0.59655344]\n",
      " [0.09178413 0.34518623 0.6627525  0.44171348 0.5514878  0.70371246\n",
      "  0.58940125 0.04993276 0.56179184 0.7663585 ]\n",
      " [0.91090834 0.09290995 0.9025214  0.46096042 0.45201847 0.9994255\n",
      "  0.16242374 0.7093706  0.16062407 0.81077677]\n",
      " [0.03514718 0.5348867  0.16650012 0.30841038 0.04506241 0.23857613\n",
      "  0.67483455 0.7823827  0.69520164 0.32895446]\n",
      " [0.49403188 0.52412134 0.29854125 0.46310815 0.9847843  0.50113493\n",
      "  0.39807245 0.72790533 0.86333096 0.02616954]]\n",
      "\n",
      "It is of type: <class 'jax.interpreters.xla.DeviceArray'>\n"
     ]
    }
   ],
   "source": [
    "trax_numpy_array = np.array(numpy_array)\n",
    "print(f\"The trax numpy array looks like this:\\n\\n {trax_numpy_array}\\n\")\n",
    "print(f\"It is of type: {type(trax_numpy_array)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The perplexity is a metric that measures how well a probability model predicts a sample and it is commonly used to evaluate language models. It is defined as: \n",
    "\n",
    "$$P(W) = \\sqrt[N]{\\prod_{i=1}^{N} \\frac{1}{P(w_i| w_1,...,w_{n-1})}}$$\n",
    "\n",
    "As an implementation hack, you would usually take the log of that formula (to enable us to use the log probabilities we get as output of our `RNN`, convert exponents to products, and products into sums which makes computations less complicated and computationally more efficient). You should also take care of the padding, since you do not want to include the padding when calculating the perplexity (because we do not want to have a perplexity measure artificially good). The algebra behind this process is explained next:\n",
    "\n",
    "\n",
    "$$log P(W) = {log\\big(\\sqrt[N]{\\prod_{i=1}^{N} \\frac{1}{P(w_i| w_1,...,w_{n-1})}}\\big)}$$\n",
    "\n",
    "$$ = {log\\big({\\prod_{i=1}^{N} \\frac{1}{P(w_i| w_1,...,w_{n-1})}}\\big)^{\\frac{1}{N}}}$$ \n",
    "\n",
    "$$ = {log\\big({\\prod_{i=1}^{N}{P(w_i| w_1,...,w_{n-1})}}\\big)^{-\\frac{1}{N}}} $$\n",
    "$$ = -\\frac{1}{N}{log\\big({\\prod_{i=1}^{N}{P(w_i| w_1,...,w_{n-1})}}\\big)} $$\n",
    "$$ = -\\frac{1}{N}{\\big({\\sum_{i=1}^{N}{logP(w_i| w_1,...,w_{n-1})}}\\big)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example is made up of:\n",
    "   - `predictions` : batch of tensors corresponding to lines of text predicted by the model.\n",
    "   - `targets` : batch of actual tensors corresponding to lines of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trax import layers as tl\n",
    "\n",
    "# Load from .npy files\n",
    "predictions = numpy.load('predictions.npy')\n",
    "targets = numpy.load('targets.npy')\n",
    "\n",
    "# Cast to jax.interpreters.xla.DeviceArray\n",
    "predictions = np.array(predictions)\n",
    "targets = np.array(targets)\n",
    "\n",
    "# Print shapes\n",
    "print(f'predictions has shape: {predictions.shape}')\n",
    "print(f'targets has shape: {targets.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshaped_targets = tl.one_hot(targets, predictions.shape[-1]) #trax's one_hot function takes the input as one_hot(x, n_categories, dtype=optional)\n",
    "print(f'reshaped_targets has shape: {reshaped_targets.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By calculating the product of the predictions and the reshaped targets and summing across the last dimension, the total log perplexity can be computed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_log_ppx = np.sum(predictions * reshaped_targets, axis= -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you will need to account for the padding so this metric is not artificially deflated (since a lower perplexity means a better model). For identifying which elements are padding and which are not, you can use `np.equal()` and get a tensor with `1s` in the positions of actual values and `0s` where there are paddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "non_pad = 1.0 - np.equal(targets, 0)\n",
    "print(f'non_pad has shape: {non_pad.shape}\\n')\n",
    "print(f'non_pad looks like this: \\n\\n {non_pad}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By computing the product of the total log perplexity and the non_pad tensor we remove the effect of padding on the metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_log_ppx = total_log_ppx * non_pad\n",
    "print(f'real perplexity still has shape: {real_log_ppx.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check the effect of filtering out the padding by looking at the two log perplexity tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'log perplexity tensor before filtering padding: \\n\\n {total_log_ppx}\\n')\n",
    "print(f'log perplexity tensor after filtering padding: \\n\\n {real_log_ppx}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a single average log perplexity across all the elements in the batch you can sum across both dimensions and divide by the number of elements. Notice that the result will be the negative of the real log perplexity of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_ppx = np.sum(real_log_ppx) / np.sum(non_pad)\n",
    "log_ppx = -log_ppx\n",
    "print(f'The log perplexity and perplexity of the model are respectively: {log_ppx} and {np.exp(log_ppx)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
